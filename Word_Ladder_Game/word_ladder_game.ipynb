{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we need a list of valid words.\n",
    "#get the dictionary of words from some url like this.\n",
    "\n",
    "import urllib3\n",
    "http=urllib3.PoolManager()\n",
    "r=http.request('GET','https://raw.githubusercontent.com/dwyl/english-words/master/words.txt',preload_content=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4862992"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"words.txt\",'wb') as out:\n",
    "    data=r.read()\n",
    "    out.write(data)\n",
    "r.release_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets clean up and remove line endings from each word in the file.\n",
    "\n",
    "word_list=open('words.txt').readlines()\n",
    "word_list=map(str.strip, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1018"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want words of length 3,so lets select those words.\n",
    "#we will also eliminate words which start with upper-case or contain alpha-numeric characters.\n",
    "#finally we will lower-case everything.\n",
    "\n",
    "word_list=[word for word in word_list if len(word)==3]\n",
    "word_list=[word for word in word_list if word[0].islower()]\n",
    "word_list=[word for word in word_list if word.isalpha()]\n",
    "word_list=list(map(str.lower,word_list))\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aah',\n",
       " 'aal',\n",
       " 'abb',\n",
       " 'abd',\n",
       " 'aby',\n",
       " 'abl',\n",
       " 'abn',\n",
       " 'abr',\n",
       " 'abt',\n",
       " 'abv',\n",
       " 'acy',\n",
       " 'ade',\n",
       " 'ady',\n",
       " 'adj',\n",
       " 'ado',\n",
       " 'adv',\n",
       " 'adz',\n",
       " 'aeq',\n",
       " 'aer',\n",
       " 'afd',\n",
       " 'aff',\n",
       " 'aga',\n",
       " 'age',\n",
       " 'agy',\n",
       " 'ago',\n",
       " 'ahi',\n",
       " 'aho',\n",
       " 'ahs',\n",
       " 'ahu',\n",
       " 'aye',\n",
       " 'aik',\n",
       " 'ail',\n",
       " 'ays',\n",
       " 'ait',\n",
       " 'ayu',\n",
       " 'ake',\n",
       " 'ako',\n",
       " 'aku',\n",
       " 'ale',\n",
       " 'aly',\n",
       " 'alk',\n",
       " 'all',\n",
       " 'aln',\n",
       " 'alt',\n",
       " 'alw',\n",
       " 'ana',\n",
       " 'and',\n",
       " 'ane',\n",
       " 'ans',\n",
       " 'ant',\n",
       " 'aob',\n",
       " 'aor',\n",
       " 'aph',\n",
       " 'apx',\n",
       " 'arb',\n",
       " 'ard',\n",
       " 'ary',\n",
       " 'arn',\n",
       " 'arr',\n",
       " 'art',\n",
       " 'arx',\n",
       " 'asb',\n",
       " 'ase',\n",
       " 'ast',\n",
       " 'ate',\n",
       " 'aud',\n",
       " 'auf',\n",
       " 'auh',\n",
       " 'aul',\n",
       " 'avg',\n",
       " 'avn',\n",
       " 'awa',\n",
       " 'awd',\n",
       " 'awe',\n",
       " 'awm',\n",
       " 'awn',\n",
       " 'azo',\n",
       " 'bad',\n",
       " 'bah',\n",
       " 'baw',\n",
       " 'bbs',\n",
       " 'bcf',\n",
       " 'bdl',\n",
       " 'bec',\n",
       " 'beg',\n",
       " 'bey',\n",
       " 'bet',\n",
       " 'bhd',\n",
       " 'bye',\n",
       " 'big',\n",
       " 'bin',\n",
       " 'bio',\n",
       " 'byp',\n",
       " 'bys',\n",
       " 'biz',\n",
       " 'bkg',\n",
       " 'bks',\n",
       " 'bkt',\n",
       " 'bld',\n",
       " 'ble',\n",
       " 'blk',\n",
       " 'blo',\n",
       " 'boa',\n",
       " 'boe',\n",
       " 'bog',\n",
       " 'boh',\n",
       " 'boo',\n",
       " 'bpt',\n",
       " 'brr',\n",
       " 'bsh',\n",
       " 'buy',\n",
       " 'bun',\n",
       " 'buz',\n",
       " 'bvt',\n",
       " 'bxs',\n",
       " 'cag',\n",
       " 'cay',\n",
       " 'caw',\n",
       " 'ccm',\n",
       " 'cdg',\n",
       " 'cee',\n",
       " 'cep',\n",
       " 'cfh',\n",
       " 'cfm',\n",
       " 'cfs',\n",
       " 'chg',\n",
       " 'chn',\n",
       " 'chs',\n",
       " 'cyc',\n",
       " 'cig',\n",
       " 'cyl',\n",
       " 'cyp',\n",
       " 'cir',\n",
       " 'civ',\n",
       " 'ckw',\n",
       " 'cle',\n",
       " 'clk',\n",
       " 'cog',\n",
       " 'cow',\n",
       " 'coz',\n",
       " 'cre',\n",
       " 'cro',\n",
       " 'cru',\n",
       " 'csk',\n",
       " 'csw',\n",
       " 'cte',\n",
       " 'ctf',\n",
       " 'ctg',\n",
       " 'ctn',\n",
       " 'ctr',\n",
       " 'cud',\n",
       " 'cue',\n",
       " 'cuj',\n",
       " 'cum',\n",
       " 'cun',\n",
       " 'cup',\n",
       " 'cur',\n",
       " 'cwm',\n",
       " 'cwt',\n",
       " 'dah',\n",
       " 'dau',\n",
       " 'daw',\n",
       " 'dbl',\n",
       " 'dbv',\n",
       " 'dbw',\n",
       " 'def',\n",
       " 'deg',\n",
       " 'dep',\n",
       " 'dha',\n",
       " 'dye',\n",
       " 'dig',\n",
       " 'dyn',\n",
       " 'dys',\n",
       " 'dit',\n",
       " 'dkg',\n",
       " 'dkl',\n",
       " 'dkm',\n",
       " 'dks',\n",
       " 'dlr',\n",
       " 'dog',\n",
       " 'doh',\n",
       " 'dom',\n",
       " 'doo',\n",
       " 'doz',\n",
       " 'dpt',\n",
       " 'dtd',\n",
       " 'dub',\n",
       " 'due',\n",
       " 'dug',\n",
       " 'dum',\n",
       " 'duo',\n",
       " 'dup',\n",
       " 'dux',\n",
       " 'dwt',\n",
       " 'dzo',\n",
       " 'ead',\n",
       " 'ean',\n",
       " 'ear',\n",
       " 'eat',\n",
       " 'ebb',\n",
       " 'ecb',\n",
       " 'edh',\n",
       " 'een',\n",
       " 'eer',\n",
       " 'eff',\n",
       " 'efl',\n",
       " 'eft',\n",
       " 'egg',\n",
       " 'eye',\n",
       " 'eyl',\n",
       " 'ein',\n",
       " 'eyn',\n",
       " 'eir',\n",
       " 'eyr',\n",
       " 'eke',\n",
       " 'elb',\n",
       " 'eld',\n",
       " 'elk',\n",
       " 'els',\n",
       " 'eme',\n",
       " 'enc',\n",
       " 'end',\n",
       " 'ene',\n",
       " 'enl',\n",
       " 'ent',\n",
       " 'env',\n",
       " 'eon',\n",
       " 'epi',\n",
       " 'erf',\n",
       " 'erg',\n",
       " 'ery',\n",
       " 'erk',\n",
       " 'err',\n",
       " 'ese',\n",
       " 'ess',\n",
       " 'est',\n",
       " 'eth',\n",
       " 'ety',\n",
       " 'evg',\n",
       " 'exp',\n",
       " 'exr',\n",
       " 'ext',\n",
       " 'fac',\n",
       " 'fcy',\n",
       " 'fcp',\n",
       " 'feh',\n",
       " 'fei',\n",
       " 'fer',\n",
       " 'feu',\n",
       " 'few',\n",
       " 'fgn',\n",
       " 'fic',\n",
       " 'fid',\n",
       " 'fie',\n",
       " 'fig',\n",
       " 'fil',\n",
       " 'fix',\n",
       " 'fiz',\n",
       " 'fld',\n",
       " 'fly',\n",
       " 'fll',\n",
       " 'flu',\n",
       " 'fmt',\n",
       " 'fod',\n",
       " 'fog',\n",
       " 'foh',\n",
       " 'fol',\n",
       " 'foo',\n",
       " 'fop',\n",
       " 'fou',\n",
       " 'fow',\n",
       " 'fro',\n",
       " 'frt',\n",
       " 'fth',\n",
       " 'fub',\n",
       " 'fug',\n",
       " 'ful',\n",
       " 'fum',\n",
       " 'fun',\n",
       " 'fut',\n",
       " 'gaj',\n",
       " 'gam',\n",
       " 'gap',\n",
       " 'gau',\n",
       " 'gaz',\n",
       " 'gea',\n",
       " 'gey',\n",
       " 'gen',\n",
       " 'get',\n",
       " 'gez',\n",
       " 'ggr',\n",
       " 'ghi',\n",
       " 'gie',\n",
       " 'gye',\n",
       " 'gig',\n",
       " 'gim',\n",
       " 'gym',\n",
       " 'gyn',\n",
       " 'gip',\n",
       " 'gyp',\n",
       " 'git',\n",
       " 'glb',\n",
       " 'glt',\n",
       " 'gns',\n",
       " 'gob',\n",
       " 'goi',\n",
       " 'goy',\n",
       " 'gol',\n",
       " 'gon',\n",
       " 'goo',\n",
       " 'got',\n",
       " 'gou',\n",
       " 'gph',\n",
       " 'gra',\n",
       " 'gre',\n",
       " 'grf',\n",
       " 'gry',\n",
       " 'grr',\n",
       " 'grx',\n",
       " 'gtd',\n",
       " 'gud',\n",
       " 'gue',\n",
       " 'gul',\n",
       " 'gup',\n",
       " 'guv',\n",
       " 'guz',\n",
       " 'hae',\n",
       " 'haf',\n",
       " 'hah',\n",
       " 'haj',\n",
       " 'has',\n",
       " 'hav',\n",
       " 'haw',\n",
       " 'hcb',\n",
       " 'hed',\n",
       " 'hee',\n",
       " 'heh',\n",
       " 'hei',\n",
       " 'her',\n",
       " 'hes',\n",
       " 'het',\n",
       " 'hex',\n",
       " 'hgt',\n",
       " 'hia',\n",
       " 'hid',\n",
       " 'hyd',\n",
       " 'hie',\n",
       " 'hin',\n",
       " 'hip',\n",
       " 'hyp',\n",
       " 'hir',\n",
       " 'hit',\n",
       " 'hld',\n",
       " 'hmm',\n",
       " 'hny',\n",
       " 'hoc',\n",
       " 'hod',\n",
       " 'hog',\n",
       " 'hop',\n",
       " 'hor',\n",
       " 'hot',\n",
       " 'hox',\n",
       " 'hrs',\n",
       " 'hub',\n",
       " 'hug',\n",
       " 'huh',\n",
       " 'hup',\n",
       " 'hvy',\n",
       " 'hwa',\n",
       " 'hwy',\n",
       " 'hwt',\n",
       " 'yad',\n",
       " 'yah',\n",
       " 'yay',\n",
       " 'yak',\n",
       " 'ial',\n",
       " 'ian',\n",
       " 'iao',\n",
       " 'yar',\n",
       " 'yas',\n",
       " 'yat',\n",
       " 'yaw',\n",
       " 'ice',\n",
       " 'ich',\n",
       " 'icy',\n",
       " 'ick',\n",
       " 'ics',\n",
       " 'ide',\n",
       " 'yds',\n",
       " 'yea',\n",
       " 'yed',\n",
       " 'yep',\n",
       " 'ier',\n",
       " 'yer',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'yew',\n",
       " 'yex',\n",
       " 'yez',\n",
       " 'ify',\n",
       " 'ign',\n",
       " 'ihi',\n",
       " 'iii',\n",
       " 'iyo',\n",
       " 'yip',\n",
       " 'yis',\n",
       " 'ile',\n",
       " 'ilk',\n",
       " 'imi',\n",
       " 'imu',\n",
       " 'ina',\n",
       " 'ine',\n",
       " 'yne',\n",
       " 'ing',\n",
       " 'ink',\n",
       " 'inv',\n",
       " 'yod',\n",
       " 'yoe',\n",
       " 'yoi',\n",
       " 'yoy',\n",
       " 'yok',\n",
       " 'yom',\n",
       " 'ion',\n",
       " 'yon',\n",
       " 'ior',\n",
       " 'yor',\n",
       " 'yot',\n",
       " 'you',\n",
       " 'yow',\n",
       " 'yox',\n",
       " 'iph',\n",
       " 'ipr',\n",
       " 'iqs',\n",
       " 'irk',\n",
       " 'ise',\n",
       " 'ish',\n",
       " 'ism',\n",
       " 'ist',\n",
       " 'isz',\n",
       " 'itd',\n",
       " 'ite',\n",
       " 'ity',\n",
       " 'yug',\n",
       " 'yuh',\n",
       " 'ium',\n",
       " 'yum',\n",
       " 'yus',\n",
       " 'ive',\n",
       " 'iwa',\n",
       " 'ize',\n",
       " 'jab',\n",
       " 'jad',\n",
       " 'jah',\n",
       " 'jai',\n",
       " 'jak',\n",
       " 'jar',\n",
       " 'jaw',\n",
       " 'jct',\n",
       " 'jee',\n",
       " 'jef',\n",
       " 'jeg',\n",
       " 'jib',\n",
       " 'jig',\n",
       " 'jin',\n",
       " 'jnd',\n",
       " 'jnt',\n",
       " 'jog',\n",
       " 'jot',\n",
       " 'jow',\n",
       " 'jug',\n",
       " 'jus',\n",
       " 'jut',\n",
       " 'juv',\n",
       " 'kab',\n",
       " 'kae',\n",
       " 'kaj',\n",
       " 'kci',\n",
       " 'ked',\n",
       " 'kef',\n",
       " 'kep',\n",
       " 'kex',\n",
       " 'kgf',\n",
       " 'kgr',\n",
       " 'khi',\n",
       " 'khu',\n",
       " 'kie',\n",
       " 'kye',\n",
       " 'kil',\n",
       " 'kin',\n",
       " 'kir',\n",
       " 'kyu',\n",
       " 'kln',\n",
       " 'kmc',\n",
       " 'kob',\n",
       " 'koe',\n",
       " 'koi',\n",
       " 'kon',\n",
       " 'kop',\n",
       " 'kor',\n",
       " 'kou',\n",
       " 'kpc',\n",
       " 'kph',\n",
       " 'krs',\n",
       " 'ksi',\n",
       " 'kue',\n",
       " 'kva',\n",
       " 'kwh',\n",
       " 'lag',\n",
       " 'lah',\n",
       " 'lav',\n",
       " 'lbf',\n",
       " 'lbw',\n",
       " 'leg',\n",
       " 'lei',\n",
       " 'lep',\n",
       " 'let',\n",
       " 'leu',\n",
       " 'lex',\n",
       " 'lez',\n",
       " 'lhb',\n",
       " 'lyc',\n",
       " 'lid',\n",
       " 'lye',\n",
       " 'lig',\n",
       " 'lym',\n",
       " 'lip',\n",
       " 'liq',\n",
       " 'lis',\n",
       " 'lnr',\n",
       " 'log',\n",
       " 'loo',\n",
       " 'loq',\n",
       " 'lor',\n",
       " 'lpm',\n",
       " 'lpw',\n",
       " 'ltr',\n",
       " 'lub',\n",
       " 'luc',\n",
       " 'lue',\n",
       " 'lui',\n",
       " 'lum',\n",
       " 'luv',\n",
       " 'mas',\n",
       " 'maw',\n",
       " 'mbd',\n",
       " 'mcg',\n",
       " 'mem',\n",
       " 'men',\n",
       " 'meq',\n",
       " 'mer',\n",
       " 'meu',\n",
       " 'mew',\n",
       " 'mfd',\n",
       " 'mgd',\n",
       " 'mho',\n",
       " 'myc',\n",
       " 'myg',\n",
       " 'mym',\n",
       " 'mix',\n",
       " 'mkt',\n",
       " 'mlx',\n",
       " 'mmf',\n",
       " 'mob',\n",
       " 'moy',\n",
       " 'mom',\n",
       " 'moo',\n",
       " 'mow',\n",
       " 'mpb',\n",
       " 'mtg',\n",
       " 'mtn',\n",
       " 'mud',\n",
       " 'mug',\n",
       " 'mum',\n",
       " 'mut',\n",
       " 'mxd',\n",
       " 'nae',\n",
       " 'naf',\n",
       " 'nav',\n",
       " 'naw',\n",
       " 'nci',\n",
       " 'nef',\n",
       " 'nek',\n",
       " 'nib',\n",
       " 'nid',\n",
       " 'nik',\n",
       " 'nit',\n",
       " 'nix',\n",
       " 'noa',\n",
       " 'nob',\n",
       " 'nog',\n",
       " 'noy',\n",
       " 'nol',\n",
       " 'nom',\n",
       " 'non',\n",
       " 'noo',\n",
       " 'not',\n",
       " 'nou',\n",
       " 'now',\n",
       " 'nub',\n",
       " 'oad',\n",
       " 'oaf',\n",
       " 'oam',\n",
       " 'oar',\n",
       " 'obb',\n",
       " 'obi',\n",
       " 'obj',\n",
       " 'obl',\n",
       " 'obv',\n",
       " 'och',\n",
       " 'ock',\n",
       " 'ode',\n",
       " 'oer',\n",
       " 'off',\n",
       " 'oft',\n",
       " 'oho',\n",
       " 'ohs',\n",
       " 'ohv',\n",
       " 'oic',\n",
       " 'oid',\n",
       " 'oie',\n",
       " 'oii',\n",
       " 'oik',\n",
       " 'oil',\n",
       " 'oke',\n",
       " 'oki',\n",
       " 'ole',\n",
       " 'olp',\n",
       " 'oma',\n",
       " 'ome',\n",
       " 'one',\n",
       " 'ony',\n",
       " 'ons',\n",
       " 'ont',\n",
       " 'oof',\n",
       " 'ooh',\n",
       " 'oon',\n",
       " 'oos',\n",
       " 'ope',\n",
       " 'opt',\n",
       " 'orc',\n",
       " 'orf',\n",
       " 'org',\n",
       " 'ory',\n",
       " 'orl',\n",
       " 'ors',\n",
       " 'ose',\n",
       " 'ote',\n",
       " 'oud',\n",
       " 'ouf',\n",
       " 'oui',\n",
       " 'our',\n",
       " 'ous',\n",
       " 'out',\n",
       " 'owd',\n",
       " 'owe',\n",
       " 'owk',\n",
       " 'owl',\n",
       " 'own',\n",
       " 'owt',\n",
       " 'oxy',\n",
       " 'oxo',\n",
       " 'ozs',\n",
       " 'pah',\n",
       " 'pay',\n",
       " 'pav',\n",
       " 'paw',\n",
       " 'pct',\n",
       " 'pea',\n",
       " 'ped',\n",
       " 'pee',\n",
       " 'pes',\n",
       " 'pew',\n",
       " 'pfc',\n",
       " 'pfd',\n",
       " 'pfg',\n",
       " 'pfx',\n",
       " 'pho',\n",
       " 'phr',\n",
       " 'pht',\n",
       " 'phu',\n",
       " 'pya',\n",
       " 'pie',\n",
       " 'pye',\n",
       " 'pig',\n",
       " 'pik',\n",
       " 'pil',\n",
       " 'pim',\n",
       " 'pir',\n",
       " 'pyr',\n",
       " 'pis',\n",
       " 'piu',\n",
       " 'pix',\n",
       " 'pyx',\n",
       " 'pkg',\n",
       " 'pks',\n",
       " 'pkt',\n",
       " 'plf',\n",
       " 'pli',\n",
       " 'ply',\n",
       " 'plu',\n",
       " 'pmk',\n",
       " 'pod',\n",
       " 'poi',\n",
       " 'poy',\n",
       " 'pon',\n",
       " 'pot',\n",
       " 'pox',\n",
       " 'poz',\n",
       " 'pph',\n",
       " 'ppl',\n",
       " 'ppr',\n",
       " 'prf',\n",
       " 'prn',\n",
       " 'prp',\n",
       " 'ptg',\n",
       " 'pty',\n",
       " 'pts',\n",
       " 'pua',\n",
       " 'pub',\n",
       " 'pug',\n",
       " 'puy',\n",
       " 'pun',\n",
       " 'pus',\n",
       " 'put',\n",
       " 'pwr',\n",
       " 'pwt',\n",
       " 'qaf',\n",
       " 'qat',\n",
       " 'qrs',\n",
       " 'qtd',\n",
       " 'qto',\n",
       " 'qtr',\n",
       " 'qts',\n",
       " 'qua',\n",
       " 'qui',\n",
       " 'quo',\n",
       " 'rag',\n",
       " 'rah',\n",
       " 'rat',\n",
       " 'raw',\n",
       " 'rax',\n",
       " 'rcd',\n",
       " 'red',\n",
       " 'ref',\n",
       " 'reh',\n",
       " 'rei',\n",
       " 'rel',\n",
       " 'req',\n",
       " 'res',\n",
       " 'ret',\n",
       " 'rfb',\n",
       " 'rfz',\n",
       " 'rhb',\n",
       " 'rhd',\n",
       " 'rhe',\n",
       " 'rho',\n",
       " 'rya',\n",
       " 'rib',\n",
       " 'rig',\n",
       " 'rin',\n",
       " 'riv',\n",
       " 'rix',\n",
       " 'rle',\n",
       " 'rly',\n",
       " 'rnd',\n",
       " 'roo',\n",
       " 'rpt',\n",
       " 'rte',\n",
       " 'rti',\n",
       " 'rtw',\n",
       " 'rub',\n",
       " 'rud',\n",
       " 'rug',\n",
       " 'run',\n",
       " 'rut',\n",
       " 'rux',\n",
       " 'rwd',\n",
       " 'sah',\n",
       " 'saj',\n",
       " 'sav',\n",
       " 'sea',\n",
       " 'seg',\n",
       " 'sey',\n",
       " 'seq',\n",
       " 'sew',\n",
       " 'sfm',\n",
       " 'sfz',\n",
       " 'sgd',\n",
       " 'she',\n",
       " 'shh',\n",
       " 'shi',\n",
       " 'shy',\n",
       " 'sho',\n",
       " 'shp',\n",
       " 'shr',\n",
       " 'sht',\n",
       " 'sie',\n",
       " 'sye',\n",
       " 'sym',\n",
       " 'sis',\n",
       " 'ska',\n",
       " 'ski',\n",
       " 'sld',\n",
       " 'slt',\n",
       " 'sml',\n",
       " 'sny',\n",
       " 'sod',\n",
       " 'soe',\n",
       " 'sog',\n",
       " 'soy',\n",
       " 'sok',\n",
       " 'sot',\n",
       " 'sov',\n",
       " 'sox',\n",
       " 'spy',\n",
       " 'spp',\n",
       " 'spt',\n",
       " 'sqd',\n",
       " 'sqq',\n",
       " 'ssu',\n",
       " 'stg',\n",
       " 'sty',\n",
       " 'stk',\n",
       " 'str',\n",
       " 'sub',\n",
       " 'sud',\n",
       " 'suf',\n",
       " 'suq',\n",
       " 'suz',\n",
       " 'swy',\n",
       " 'taa',\n",
       " 'taj',\n",
       " 'taw',\n",
       " 'tax',\n",
       " 'tch',\n",
       " 'tck',\n",
       " 'tea',\n",
       " 'tee',\n",
       " 'tef',\n",
       " 'teg',\n",
       " 'tet',\n",
       " 'tew',\n",
       " 'tez',\n",
       " 'tfr',\n",
       " 'tgt',\n",
       " 'tha',\n",
       " 'the',\n",
       " 'thy',\n",
       " 'tib',\n",
       " 'tie',\n",
       " 'tig',\n",
       " 'tyg',\n",
       " 'til',\n",
       " 'tin',\n",
       " 'typ',\n",
       " 'tis',\n",
       " 'tyt',\n",
       " 'tji',\n",
       " 'tkt',\n",
       " 'tlo',\n",
       " 'tlr',\n",
       " 'tmh',\n",
       " 'toa',\n",
       " 'toe',\n",
       " 'tog',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'too',\n",
       " 'tot',\n",
       " 'tou',\n",
       " 'tov',\n",
       " 'tox',\n",
       " 'tpd',\n",
       " 'tph',\n",
       " 'tpk',\n",
       " 'tra',\n",
       " 'trf',\n",
       " 'try',\n",
       " 'trp',\n",
       " 'trs',\n",
       " 'trt',\n",
       " 'tsk',\n",
       " 'tua',\n",
       " 'tub',\n",
       " 'tue',\n",
       " 'tug',\n",
       " 'tui',\n",
       " 'tuy',\n",
       " 'tum',\n",
       " 'tun',\n",
       " 'tup',\n",
       " 'tux',\n",
       " 'two',\n",
       " 'twp',\n",
       " 'txt',\n",
       " 'ubi',\n",
       " 'udo',\n",
       " 'ugh',\n",
       " 'ugt',\n",
       " 'uhs',\n",
       " 'uji',\n",
       " 'ule',\n",
       " 'ult',\n",
       " 'umm',\n",
       " 'ump',\n",
       " 'umu',\n",
       " 'unb',\n",
       " 'unc',\n",
       " 'ung',\n",
       " 'unl',\n",
       " 'unn',\n",
       " 'unp',\n",
       " 'uns',\n",
       " 'upo',\n",
       " 'ura',\n",
       " 'urb',\n",
       " 'ure',\n",
       " 'urf',\n",
       " 'urn',\n",
       " 'urs',\n",
       " 'ush',\n",
       " 'ust',\n",
       " 'usu',\n",
       " 'uti',\n",
       " 'uts',\n",
       " 'uva',\n",
       " 'vag',\n",
       " 'vau',\n",
       " 'vaw',\n",
       " 'veg',\n",
       " 'vei',\n",
       " 'vel',\n",
       " 'vet',\n",
       " 'vie',\n",
       " 'vii',\n",
       " 'vil',\n",
       " 'vim',\n",
       " 'vis',\n",
       " 'viz',\n",
       " 'voc',\n",
       " 'vog',\n",
       " 'vol',\n",
       " 'vow',\n",
       " 'vox',\n",
       " 'vug',\n",
       " 'vum',\n",
       " 'wab',\n",
       " 'wad',\n",
       " 'wae',\n",
       " 'wah',\n",
       " 'way',\n",
       " 'was',\n",
       " 'waw',\n",
       " 'wax',\n",
       " 'wee',\n",
       " 'wef',\n",
       " 'wem',\n",
       " 'wen',\n",
       " 'wer',\n",
       " 'wet',\n",
       " 'wha',\n",
       " 'whf',\n",
       " 'why',\n",
       " 'whr',\n",
       " 'whs',\n",
       " 'wid',\n",
       " 'wim',\n",
       " 'wir',\n",
       " 'wiz',\n",
       " 'wjc',\n",
       " 'wmk',\n",
       " 'woa',\n",
       " 'wob',\n",
       " 'wod',\n",
       " 'woe',\n",
       " 'wog',\n",
       " 'woy',\n",
       " 'wok',\n",
       " 'wop',\n",
       " 'wos',\n",
       " 'wot',\n",
       " 'wpm',\n",
       " 'wry',\n",
       " 'wro',\n",
       " 'wud',\n",
       " 'wup',\n",
       " 'wur',\n",
       " 'wus',\n",
       " 'wut',\n",
       " 'xat',\n",
       " 'xcl',\n",
       " 'xed',\n",
       " 'xii',\n",
       " 'xis',\n",
       " 'xiv',\n",
       " 'xix',\n",
       " 'xyz',\n",
       " 'xvi',\n",
       " 'xxi',\n",
       " 'xxv',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these 1018 words will become a node in our graph and we will create edges connecting the node associated with each pair of words which differ by only one letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U3')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "word_list=np.asarray(word_list)\n",
    "word_list.dtype\n",
    "word_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list.itemsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an array where each entry is three unicode characters long. We did like to find all pairs where exactly one character is different. We will start by converting each word to a three dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1018, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_bytes=np.ndarray((word_list.size,word_list.itemsize),dtype='uint8',buffer=word_list.data)\n",
    "\n",
    "#Each unicode character is four bytes long.\n",
    "#We only need first byte \n",
    "#we know there are three characters per word\n",
    "\n",
    "word_bytes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 97,   0,   0, ...,   0,   0,   0],\n",
       "       [ 97,   0,   0, ...,   0,   0,   0],\n",
       "       [ 97,   0,   0, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [122,   0,   0, ...,   0,   0,   0],\n",
       "       [122,   0,   0, ...,   0,   0,   0],\n",
       "       [122,   0,   0, ...,   0,   0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1018, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['aah', 'aal', 'abb', 'abd'], dtype='<U3')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 97,  97, 104],\n",
       "       [ 97,  97, 108],\n",
       "       [ 97,  98,  98],\n",
       "       [ 97,  98, 100]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_bytes=word_bytes[:,::word_list.itemsize//3]\n",
    "word_bytes.shape\n",
    "word_list[0:4]\n",
    "word_bytes[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the Hamming Distance between each point to determine which pair of words are connected.The Hamming distance measures the fraction of entries between two vectors which differ: any two words with a Hamming distance equal to 1/N, where N is the number of letters, are connected in the word ladder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517653,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "517653"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "#pdist: pairwise distance between objects in n-D space.\n",
    "\n",
    "hamming_dist=pdist(word_bytes,metric='hamming')\n",
    "hamming_dist.shape\n",
    "(1018*1018-1018)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.33333333, 0.66666667, 0.66666667],\n",
       "       [0.33333333, 0.        , 0.66666667, 0.66666667],\n",
       "       [0.66666667, 0.66666667, 0.        , 0.33333333],\n",
       "       [0.66666667, 0.66666667, 0.33333333, 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1018, 1018)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, False],\n",
       "       [ True, False, False, False],\n",
       "       [False, False, False,  True],\n",
       "       [False, False,  True, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "#Squareform: converts a vector-form distance vector to a squareform distance matrix\n",
    "squareform(hamming_dist)[0:4,0:4]\n",
    "\n",
    "#there are three characters in each word\n",
    "mat=squareform(hamming_dist <1.5/3)\n",
    "mat.shape\n",
    "mat[0:4,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "graph=csr_matrix(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our graph is set up, we will use a sortest path search to find the path between any to words in the graph.All we need is to find the shortest path between these two indices in the graph. We'll use Dijkstra's algorithm, because it allows us to find the path for just one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1=word_list.searchsorted('and')\n",
    "i2=word_list.searchsorted('dog')\n",
    "word_list[i1]\n",
    "word_list[i2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse.csgraph import dijkstra\n",
    "distances, predecessors = dijkstra(graph,indices=i1,return_predecessors=True)\n",
    "distances[i2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the shortest path between \"and\" and \"dog\" contains only five steps. We can use the predecessors returned by the algorithm to reconstruct this path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'aud', 'mud', 'mug', 'dug', 'dog']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=[]\n",
    "i=i2\n",
    "while i != i1:\n",
    "    path.append(word_list[i])\n",
    "    i=predecessors[i]\n",
    "path.append(word_list[i1])\n",
    "path[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thats all about creating a word ladder game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some more fancier things:\n",
    "- Are there three-letter words which are not linked in a word ladder?<br>\n",
    "this is a question of connected components in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1018,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse.csgraph import connected_components\n",
    "N_components, component_list = connected_components(graph)\n",
    "N_components\n",
    "component_list.shape\n",
    "component_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1010, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.sum(component_list == i) for i in range(N_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bcf'], ['bdl'], ['epi'], ['mmf'], ['sml'], ['wjc'], ['xcl'], ['xyz']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[list(word_list[np.where(component_list == i)]) for i in range(1,N_components)]\n",
    "#these are all the three-letter words which do not connect to others via a word ladder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which words are maximally separated? Which two words take the most links to connect?<br>\n",
    "- We can determine this by computing the matrix of all shotest paths.\n",
    "- Note that by convention, the distance between two non-connected points is reported to be infinity, so we will need to remove these before finding the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('dlr', 'hmm'),\n",
       " ('dlr', 'kmc'),\n",
       " ('hmm', 'dlr'),\n",
       " ('hmm', 'lbf'),\n",
       " ('hmm', 'rfz'),\n",
       " ('hmm', 'tfr'),\n",
       " ('kmc', 'dlr'),\n",
       " ('kmc', 'tfr'),\n",
       " ('lbf', 'hmm'),\n",
       " ('rfz', 'hmm'),\n",
       " ('tfr', 'hmm'),\n",
       " ('tfr', 'kmc')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, predecessors = dijkstra(graph, return_predecessors=True)\n",
    "max_distance = np.max(distances[~np.isinf(distances)])\n",
    "max_distance\n",
    "\n",
    "i1,i2=np.where(distances == max_distance)\n",
    "list(zip(word_list[i1],word_list[i2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dlr', 'tlr', 'tlo', 'blo', 'boo', 'boh', 'bsh', 'ush', 'usu', 'umu', 'umm', 'hmm']\n",
      "['dlr', 'tlr', 'tlo', 'blo', 'bld', 'bad', 'bah', 'pah', 'pph', 'kph', 'kpc', 'kmc']\n",
      "['hmm', 'umm', 'umu', 'usu', 'ust', 'ugt', 'tgt', 'tot', 'too', 'tlo', 'tlr', 'dlr']\n",
      "['hmm', 'umm', 'umu', 'usu', 'ust', 'ast', 'abt', 'abv', 'dbv', 'dbw', 'lbw', 'lbf']\n",
      "['hmm', 'umm', 'ump', 'unp', 'uns', 'ons', 'ous', 'ouf', 'suf', 'suz', 'sfz', 'rfz']\n",
      "['hmm', 'umm', 'umu', 'usu', 'ust', 'ugt', 'tgt', 'tot', 'too', 'tlo', 'tlr', 'tfr']\n",
      "['kmc', 'kpc', 'kph', 'iph', 'ich', 'ick', 'ilk', 'blk', 'blo', 'tlo', 'tlr', 'dlr']\n",
      "['kmc', 'kpc', 'kph', 'iph', 'ich', 'ick', 'ilk', 'blk', 'blo', 'tlo', 'tlr', 'tfr']\n",
      "['lbf', 'lbw', 'dbw', 'daw', 'baw', 'bah', 'bsh', 'ush', 'usu', 'umu', 'umm', 'hmm']\n",
      "['rfz', 'rfb', 'rhb', 'rho', 'sho', 'shi', 'ihi', 'imi', 'imu', 'umu', 'umm', 'hmm']\n",
      "['tfr', 'tlr', 'tlo', 'blo', 'boo', 'boh', 'bsh', 'ush', 'usu', 'umu', 'umm', 'hmm']\n",
      "['tfr', 'tlr', 'tlo', 'blo', 'bld', 'bad', 'bah', 'pah', 'pph', 'kph', 'kpc', 'kmc']\n"
     ]
    }
   ],
   "source": [
    "#list down all the long chains\n",
    "for j,i in zip(i1,i2):\n",
    "    path=[]\n",
    "    while i !=j:\n",
    "        path.append(word_list[i])\n",
    "        i = predecessors[j,i]\n",
    "    path.append(word_list[j])\n",
    "    print(path[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
